# ğŸ¯ AI-Powered Real-Time Audio Analysis Pipeline

A comprehensive **FastAPI-based real-time audio processing system** designed for live customer service call analysis. This system provides real-time speaker diarization, transcription, sentiment analysis, and emotion recognition with WebSocket streaming capabilities.

## âœ¨ Key Features

### ğŸ¤ **Audio Processing**
- Real-time microphone input capture with optimized audio buffers
- Voice Activity Detection (VAD) for efficient processing
- Multi-channel audio support with noise reduction
- Adaptive audio quality based on connection

### ğŸ‘¥ **Speaker Intelligence**
- Advanced speaker diarization (Agent vs Customer identification)
- Speaker embedding and clustering
- Real-time speaker switching detection
- Voice characteristic analysis

### ğŸ“ **Transcription & Language**
- Live speech-to-text with **Faster-Whisper** models
- Multi-language support and automatic language detection
- Real-time transcription streaming
- Confidence scoring and error correction

### ğŸ’­ **Sentiment & Emotion Analysis**
- **Dual-mode sentiment analysis**: Text-based and Voice-based
- Real-time emotion recognition from speech patterns
- Voice feature extraction (pitch, energy, speaking rate, tone)
- Conversation flow analysis and emotional trajectory tracking

### ğŸŒ **Real-Time Communication**
- WebSocket-based real-time data streaming
- Angular frontend integration
- RESTful API endpoints for control and monitoring
- Real-time dashboard with live updates

## ğŸ—ï¸ Project Architecture

```
AudioPipelineTreatment/
â”œâ”€â”€ ğŸ“ capture/                    # Audio capture and processing
â”‚   â”œâ”€â”€ audio_capture.py          # Main audio capture logic
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“ diarization/               # Speaker diarization
â”‚   â”œâ”€â”€ speaker_diarization.py   # Speaker identification
â”‚   â”œâ”€â”€ test_diarization.py      # Diarization tests
â”‚   â””â”€â”€ newdir.py
â”œâ”€â”€ ğŸ“ transcription/             # Speech-to-text
â”‚   â””â”€â”€ speech_to_text.py        # Whisper-based transcription
â”œâ”€â”€ ğŸ“ sentiment/                 # Sentiment analysis modules
â”‚   â”œâ”€â”€ SentimentFromTrans.py    # Text-based sentiment
â”‚   â”œâ”€â”€ VoiceEmotionRecognizer.py # Voice emotion analysis
â”‚   â”œâ”€â”€ VoiceSentiment.py        # Voice sentiment analysis
â”‚   â””â”€â”€ TranscriptSentiment.py   # Transcript processing
â”œâ”€â”€ ğŸ“ tests/                     # Comprehensive test suite
â”‚   â”œâ”€â”€ test_pipeline.py         # Pipeline integration tests
â”‚   â”œâ”€â”€ test_realtime_sentiment.py
â”‚   â””â”€â”€ testdiarizatio.py
â”œâ”€â”€ ğŸ“ models/                    # Pre-trained model storage
â”‚   â”œâ”€â”€ wav2vec2-lg-xlsr-en-speech-emotion-recognition/
â”‚   â””â”€â”€ faster-whisper-base.en/
â”œâ”€â”€ ğŸ“ logs/                      # Application logs
â”œâ”€â”€ ğŸ“ tmp/                       # Temporary processing files
â”œâ”€â”€ ğŸ main.py                    # FastAPI application entry
â”œâ”€â”€ ğŸ enhanced_web_realtime_pipeline.py  # Core pipeline
â”œâ”€â”€ ğŸ RealTimePipelineMic.py     # Microphone pipeline
â”œâ”€â”€ ğŸ RealTimeWavPipeline.py     # WAV file pipeline
â”œâ”€â”€ ğŸ“‹ requirements.txt           # Python dependencies
â”œâ”€â”€ ğŸ³ dockerfile                 # Docker configuration
â””â”€â”€ ğŸ“š PROCESS_FLOW_DOCUMENTATION.md  # Detailed technical docs
```

## ğŸš€ Quick Start

### Prerequisites
- **Python 3.8+** (3.12 recommended)
- **Working microphone** for real-time audio capture
- **NVIDIA GPU** (optional, for accelerated processing)
- **4GB+ RAM** for model loading
- **Internet connection** (for initial model downloads)

### Installation

1. **Clone and Navigate**
```powershell
git clone <repository-url>
cd AudioPipelineTreatment
```

2. **Create Virtual Environment**
```powershell
python -m venv venv
venv\Scripts\activate
```

3. **Install Dependencies**
```powershell
pip install -r requirements.txt
```

4. **Download Models** (Automatic on first run)
   - Faster-Whisper models will download automatically
   - Voice emotion models will be cached locally
   - Speaker diarization models will be loaded from HuggingFace

5. **Run the Application**
```powershell
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### ğŸŒ Access Points
- **API Documentation**: http://localhost:8000/docs
- **WebSocket Endpoint**: ws://localhost:8000/ws
- **Health Check**: http://localhost:8000/health

## ğŸ“¡ API Reference

### REST Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/` | Root endpoint with welcome message |
| `GET` | `/health` | System health and status check |
| `POST` | `/upload-audio` | Upload audio file for batch processing |
| `WS` | `/ws` | WebSocket for real-time audio streaming |

### WebSocket Communication

#### **Connection Flow**
1. Connect to `ws://localhost:8000/ws`
2. Send audio data as base64-encoded chunks
3. Receive real-time analysis results

#### **Message Format - Incoming (Audio Data)**
```json
{
  "type": "audio",
  "data": "base64-encoded-audio-chunk",
  "sampleRate": 16000,
  "channels": 1
}
```

#### **Message Format - Outgoing (Analysis Results)**
```json
{
  "timestamp": "2025-08-20T12:34:56",
  "speaker": "SPEAKER_00",
  "transcription": "Hello, how can I help you today?",
  "sentiment": {
    "text_sentiment": "POSITIVE",
    "text_confidence": 0.92,
    "voice_emotion": "HAPPY",
    "voice_confidence": 0.87,
    "combined_sentiment": "POSITIVE"
  },
  "voice_features": {
    "pitch": 185.23,
    "energy": 0.12,
    "speaking_rate": 0.08,
    "tone_stability": 0.76
  },
  "speaker_info": {
    "speaker_id": "SPEAKER_00",
    "speaker_type": "agent",
    "confidence": 0.94
  }
}
```

## ğŸ§ª Testing

### Run Complete Test Suite
```powershell
pytest tests/ -v
```

### Run Specific Tests
```powershell
# Test real-time sentiment analysis
pytest tests/test_realtime_sentiment.py -v

# Test pipeline integration
pytest tests/test_pipeline.py -v

# Test speaker diarization
pytest tests/testdiarizatio.py -v
```

### Manual Testing Tools
```powershell
# Test audio capture directly
python AudioCaptureTester.py

# Test microphone pipeline
python RealTimePipelineMic.py

# Test WAV file processing
python RealTimeWavPipeline.py
```

## ğŸ“Š Real-Time Output Examples

### Console Output Format
```
ğŸ• 12:01:23 | ğŸ‘¤ SPEAKER_00 | "Hello, how can I help you today?"
ğŸ“Š [12:01:23] SENTIMENT: POSITIVE (confidence=0.92)
   ğŸ“ Text: POSITIVE | ğŸµ Voice: HAPPY
   ğŸ™ï¸  Voice Features - Pitch: 185.23Hz, Energy: 0.12, Rate: 2.1 words/sec

ğŸ• 12:01:28 | ğŸ‘¤ SPEAKER_01 | "I'm having trouble with my account."
ğŸ“Š [12:01:28] SENTIMENT: FRUSTRATED (confidence=0.78)
   ğŸ“ Text: NEUTRAL | ğŸµ Voice: FRUSTRATED  
   ğŸ™ï¸  Voice Features - Pitch: 165.45Hz, Energy: 0.18, Rate: 1.8 words/sec
```

### WebSocket JSON Stream
```json
{
  "session_id": "sess_1724150456",
  "timestamp": "12:01:23",
  "analysis": {
    "speaker": {
      "id": "SPEAKER_00",
      "type": "agent",
      "confidence": 0.94
    },
    "transcription": {
      "text": "Hello, how can I help you today?",
      "confidence": 0.96,
      "language": "en"
    },
    "sentiment": {
      "overall": "POSITIVE",
      "text_based": "POSITIVE",
      "voice_based": "HAPPY",
      "confidence": 0.92
    },
    "emotions": {
      "primary": "HAPPY",
      "secondary": "CONFIDENT",
      "arousal": 0.7,
      "valence": 0.8
    },
    "voice_analytics": {
      "pitch_hz": 185.23,
      "energy_db": -12.4,
      "speaking_rate_wps": 2.1,
      "pause_ratio": 0.15,
      "tone_stability": 0.76
    }
  }
}
```

## ğŸ”§ Configuration & Customization

### Environment Variables
```powershell
# Model Configuration
WHISPER_MODEL_SIZE=base.en     # tiny, base, small, medium, large
EMOTION_MODEL_PATH=./models/   # Custom emotion model path
ENABLE_GPU=true                # Enable GPU acceleration

# Audio Settings  
SAMPLE_RATE=16000             # Audio sample rate
BUFFER_SIZE=1024              # Audio buffer size
VAD_THRESHOLD=0.5             # Voice activity detection threshold

# API Settings
HOST=0.0.0.0                  # API host
PORT=8000                     # API port
DEBUG=false                   # Debug mode
```

### Performance Tuning

#### For Real-Time Performance
- Use **GPU acceleration** when available
- Adjust `BUFFER_SIZE` based on your hardware
- Consider **smaller Whisper models** for faster processing
- Enable **VAD** to process only speech segments

#### For Accuracy
- Use **larger Whisper models** (medium/large)
- Increase **speaker diarization** sensitivity
- Enable **emotion model ensembling**

## ğŸ³ Docker Deployment

### Build and Run
```powershell
# Build the container
docker build -t audio-pipeline .

# Run with GPU support
docker run --gpus all -p 8000:8000 audio-pipeline

# Run CPU-only
docker run -p 8000:8000 audio-pipeline
```

### Docker Compose
```yaml
version: '3.8'
services:
  audio-pipeline:
    build: .
    ports:
      - "8000:8000"
    environment:
      - ENABLE_GPU=true
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

## ğŸ› ï¸ Technical Stack

### Core Technologies
- **FastAPI** - Modern, fast web framework for APIs
- **WebSockets** - Real-time bidirectional communication
- **PyTorch** - Deep learning framework for ML models
- **Faster-Whisper** - Optimized speech recognition
- **PyAnnote.Audio** - Speaker diarization and audio analysis
- **NumPy & SciPy** - Numerical computing and signal processing

### Machine Learning Models
- **Wav2Vec2-XLS-R** - Voice emotion recognition
- **Faster-Whisper-Base/Small** - Speech-to-text transcription
- **PyAnnote Speaker Diarization** - Speaker identification
- **Custom Sentiment Models** - Text and voice sentiment analysis

### Audio Processing
- **PyAudio** - Real-time audio I/O
- **LibROSA** - Audio feature extraction
- **TorchAudio** - Audio processing with PyTorch
- **Voice Activity Detection** - Efficient audio segmentation

## ğŸ” Troubleshooting

### Common Issues

#### **"No audio input detected"**
```powershell
# Check microphone permissions
# Verify audio device in Device Manager
# Test with: python AudioCaptureTester.py
```

#### **"Model download failed"**
```powershell
# Check internet connection
# Clear cache: rm -rf ~/.cache/huggingface/
# Manual download: huggingface-cli download Systran/faster-whisper-base.en
```

#### **"WebSocket connection refused"**
```powershell
# Verify server is running: curl http://localhost:8000/health
# Check firewall settings
# Ensure port 8000 is available
```

#### **"High CPU/Memory usage"**
```powershell
# Reduce model size in config
# Enable GPU acceleration
# Adjust buffer sizes
# Close other applications
```

### Performance Optimization

#### **For Low-Latency**
- Use `tiny` or `base` Whisper models
- Reduce audio buffer size
- Enable GPU processing
- Optimize VAD thresholds

#### **For High Accuracy**
- Use `medium` or `large` Whisper models
- Increase speaker diarization sensitivity
- Enable emotion model ensembling
- Use higher quality audio input

### Debug Mode
```powershell
# Enable detailed logging
export DEBUG=true
python main.py

# View logs
tail -f pipeline.log
```

## ğŸ“š Documentation

- **[PROCESS_FLOW_DOCUMENTATION.md](PROCESS_FLOW_DOCUMENTATION.md)** - Detailed technical implementation
- **[API Documentation](http://localhost:8000/docs)** - Interactive Swagger UI
- **[Model Documentation](models/README.md)** - ML model specifications
- **[Testing Guide](tests/README.md)** - Comprehensive testing procedures

## ğŸ¤ Contributing

### Development Setup
1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Install development dependencies: `pip install -r requirements-dev.txt`
4. Run tests: `pytest`
5. Commit changes: `git commit -m 'Add amazing feature'`
6. Push to branch: `git push origin feature/amazing-feature`
7. Open a Pull Request

### Code Standards
- Follow PEP 8 for Python code style
- Add docstrings to all functions and classes
- Write unit tests for new features
- Update documentation for API changes

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

## ğŸ™‹â€â™‚ï¸ Support

### Getting Help
- **Issues**: [GitHub Issues](https://github.com/hadil-sgh/AudioPipelineTreatment/issues)
- **Discussions**: [GitHub Discussions](https://github.com/hadil-sgh/AudioPipelineTreatment/discussions)
- **Documentation**: [Process Flow Documentation](PROCESS_FLOW_DOCUMENTATION.md)

### System Requirements
- **Minimum**: Python 3.8, 4GB RAM, Intel i5 or equivalent
- **Recommended**: Python 3.12, 8GB RAM, NVIDIA GPU, Intel i7 or equivalent
- **Operating Systems**: Windows 10+, macOS 10.15+, Ubuntu 18.04+

---

**ğŸš€ Ready to get started?** Run `uvicorn main:app --reload` and visit http://localhost:8000/docs to explore the API!

**ğŸ¯ Need real-time analysis?** Connect to `ws://localhost:8000/ws` and start streaming audio data!

**ğŸ“Š Want to see it in action?** Check out the [Process Flow Documentation](PROCESS_FLOW_DOCUMENTATION.md) for detailed examples! 
