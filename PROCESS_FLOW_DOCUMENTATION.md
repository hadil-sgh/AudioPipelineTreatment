# Detailed Process Flow: Start Analysis ‚Üí Stop Analysis

## Complete Code-Level Documentation

## Overview

This document provides a comprehensive step-by-step breakdown of what happens in the AI-Powered Audio Analysis Pipeline from the moment you click "Start Analysis" until you click "Stop Analysis", with **EXACT CODE REFERENCES** and **FILE LOCATIONS**.

---

## üéØ **PHASE 1: INITIALIZATION (When you click "Start Analysis")**

### Frontend (Angular Component) - **EXACT CODE FLOW**

#### Step 1: Button Click Handler

**File**: `FrontEnd/src/app/Agent/call-managment/call-managment.component.html`
**Line**: ~118

```html
<button *ngIf="!isAnalyzing" (click)="startAnalysis()" class="btn btn-danger">
  <i class="bx bx-play-circle me-1"></i>
  Start Analysis
</button>
```

#### Step 2: Component Method Execution

**File**: `FrontEnd/src/app/Agent/call-managment/call-managment.component.ts`
**Method**: `startAnalysis()` (Line ~297)

```typescript
async startAnalysis() {
  try {
    this.connectionStatus = 'Connecting...';
    this.lastError = '';

    // Connect to WebSocket
    await this.audioStreamService.connect();

    // Start audio streaming
    await this.audioStreamService.startRecording(this.currentSessionId);

    this.isAnalyzing = true;
    this.connectionStatus = 'Analyzing...';

    console.log('Enhanced analysis started');
  } catch (error) {
    console.error('Error starting analysis:', error);
    this.connectionStatus = 'Error: ' + error;
    this.lastError = 'Failed to start analysis: ' + error;
  }
}
```

#### Step 3: WebSocket Connection

**File**: `FrontEnd/src/app/services/audio-stream.service.ts`
**Method**: `connect()`

```typescript
async connect(): Promise<void> {
  return new Promise((resolve, reject) => {
    try {
      this.socket = new WebSocket('ws://localhost:8000/ws');

      this.socket.onopen = () => {
        console.log('‚úÖ WebSocket connected');
        this.isConnected = true;
        resolve();
      };

      this.socket.onmessage = (event) => {
        this.handleMessage(event);
      };

      this.socket.onerror = (error) => {
        console.error('‚ùå WebSocket error:', error);
        reject(error);
      };
    } catch (error) {
      reject(error);
    }
  });
}
```

#### Step 4: Audio Stream Initialization

**File**: `FrontEnd/src/app/services/audio-stream.service.ts`
**Method**: `startRecording(sessionId)`

```typescript
async startRecording(sessionId: string): Promise<void> {
  try {
    // Request microphone access
    this.mediaStream = await navigator.mediaDevices.getUserMedia({
      audio: {
        sampleRate: 16000,
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true
      }
    });

    // Create audio context and processor
    this.audioContext = new AudioContext({ sampleRate: 16000 });
    this.mediaStreamSource = this.audioContext.createMediaStreamSource(this.mediaStream);

    // Create script processor for audio chunks
    this.scriptProcessor = this.audioContext.createScriptProcessor(4096, 1, 1);

    // Audio processing callback
    this.scriptProcessor.onaudioprocess = (event) => {
      this.processAudioData(event.inputBuffer, sessionId);
    };

    // Connect audio pipeline
    this.mediaStreamSource.connect(this.scriptProcessor);
    this.scriptProcessor.connect(this.audioContext.destination);

    console.log('üé§ Audio recording started');
  } catch (error) {
    console.error('‚ùå Error starting recording:', error);
    throw error;
  }
}
```

### Backend Pipeline Initialization - **EXACT CODE FLOW**

#### Step 1: WebSocket Connection Handler

**File**: `AudioPipelineTreatment/main.py` or server file
**FastAPI WebSocket Endpoint**: `/ws`

```python
@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()

    # Get global pipeline instance
    pipeline = await get_pipeline()
    pipeline.set_websocket(websocket)

    try:
        async for message in websocket.iter_text():
            # Handle WebSocket messages
            await handle_websocket_message(message, pipeline)
    except WebSocketDisconnect:
        await pipeline.stop_session()
```

#### Step 2: Pipeline Session Start

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Class**: `EnhancedWebRealTimePipeline`
**Method**: `start_session()` (Line ~442)

```python
async def start_session(self, session_id: str):
    """Start a new audio processing session"""
    if self.is_running:
        logger.warning("Session already running")
        return False

    self.session_id = session_id

    # Initialize components if not already done
    if not self.stt:
        if not await self.initialize_components():
            return False

    # Recording is DISABLED (Line ~454)
    if self.recording_enabled:  # This is False
        # This block is skipped - no WAV files created
        pass

    self.is_running = True
    logger.info(f"Started session: {session_id}")

    # Send confirmation to frontend
    if self.websocket:
        await self.websocket.send_text(safe_json_dumps({
            "type": "session_started",
            "sessionId": session_id,
            "timestamp": datetime.now().isoformat()
        }))

    return True
```

#### Step 3: Component Initialization

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Method**: `initialize_components()` (Line ~393)

```python
async def initialize_components(self):
    """Initialize all audio processing components"""
    try:
        logger.info("Initializing enhanced audio processing components...")

        # 1. Initialize resampler (Line ~400)
        device_sr = 16000  # Frontend sends 16kHz
        if device_sr != self.target_sr:
            self.resampler = torchaudio.transforms.Resample(device_sr, self.target_sr)
        else:
            self.resampler = None  # No resampling needed

        # 2. Initialize Speech-to-Text (Line ~407)
        self.stt = SpeechToText(sample_rate=self.target_sr)

        # 3. Initialize Speaker Diarization (Line ~409)
        self.diarization = SpeakerDiarization(
            min_speech_duration=0.3,
            threshold=0.02,
            n_speakers=2,
            process_buffer_duration=2.0,
            device="cuda" if torch.cuda.is_available() else "cpu"
        )

        # 4. Initialize Sentiment Analysis (Line ~418)
        base_sentiment = RealTimeSentimentAnalyzer(min_words=2)
        self.sentiment_analyzer = EnhancedSentimentAnalyzer(base_sentiment)

        # 5. Initialize Voice Emotion (Line ~422)
        self.voice_analyzer = VoiceEmotionRecognizer()

        # 6. Initialize Enhanced Emotion Analyzer (Line ~425)
        self.emotion_analyzer = EmotionAnalyzer()

        logger.info("All components initialized successfully")
        return True

    except Exception as e:
        logger.error(f"Error initializing components: {e}")
        return False
```

---

## üéôÔ∏è **PHASE 2: REAL-TIME AUDIO PROCESSING (During Analysis)**

### Audio Capture Flow - **EXACT CODE IMPLEMENTATION**

#### Step 1: Microphone Data Capture

**File**: `FrontEnd/src/app/services/audio-stream.service.ts`
**Method**: `processAudioData()` - Called by ScriptProcessor

```typescript
private processAudioData(inputBuffer: AudioBuffer, sessionId: string): void {
  // Get audio data from buffer (4096 samples at 16kHz = ~256ms)
  const audioData = inputBuffer.getChannelData(0); // Float32Array

  // Convert Float32 to Int16 PCM for transmission
  const int16Array = new Int16Array(audioData.length);
  for (let i = 0; i < audioData.length; i++) {
    int16Array[i] = Math.max(-32768, Math.min(32767, audioData[i] * 32767));
  }

  // Send via WebSocket as binary data
  if (this.socket && this.socket.readyState === WebSocket.OPEN) {
    this.socket.send(int16Array.buffer);
  }
}
```

#### Step 2: Backend Audio Reception

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**WebSocket Binary Message Handler**:

```python
# In WebSocket message handler
async def handle_websocket_message(websocket, message_type, data):
    if message_type == "binary":
        # Process audio chunk
        result = await pipeline.process_audio_chunk(data)
        if result:
            await pipeline.send_result(result)
```

### Backend Processing Pipeline - **DETAILED CODE FLOW**

#### Step 1: Audio Conversion & Preprocessing

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Method**: `process_audio_chunk()` (Line ~513)

```python
async def process_audio_chunk(self, audio_bytes: bytes) -> Optional[Dict]:
    """Process audio chunk using the enhanced pipeline"""
    if not self.is_running or not all([self.stt, self.diarization, self.sentiment_analyzer, self.voice_analyzer]):
        return None

    try:
        # STEP 1: Convert audio data (Line ~521)
        audio_np = self.convert_webm_to_numpy(audio_bytes)

        if len(audio_np) == 0:
            return None

        # Skip very small chunks (Line ~526)
        if len(audio_np) < 1000:
            return None

        # STEP 2: Resample if needed (Line ~529)
        if self.resampler:
            tensor = torch.from_numpy(audio_np).unsqueeze(0)
            resampled = self.resampler(tensor).squeeze(0)
            audio_np = resampled.numpy()

        # STEP 3: Preprocess audio (Line ~535)
        audio_np = self.preprocess_audio(audio_np)
```

**Method**: `convert_webm_to_numpy()` (Line ~500)

```python
def convert_webm_to_numpy(self, audio_bytes: bytes) -> np.ndarray:
    """Convert WebM audio bytes to numpy array"""
    try:
        # Decode raw PCM data
        audio_data = np.frombuffer(audio_bytes, dtype=np.int16)

        # Convert to float32 and normalize to [-1.0, 1.0]
        audio_data = audio_data.astype(np.float32) / 32768.0

        return audio_data

    except Exception as e:
        logger.error(f"Error converting audio data: {e}")
        return np.array([])
```

**Method**: `preprocess_audio()` (Line ~512)

```python
def preprocess_audio(self, audio_data: np.ndarray) -> np.ndarray:
    """Enhanced audio preprocessing for better speech recognition"""
    try:
        # Ensure float32
        if audio_data.dtype != np.float32:
            audio_data = audio_data.astype(np.float32)

        # Remove DC offset
        audio_data = audio_data - np.mean(audio_data)

        # Normalize preserving dynamics
        max_val = np.max(np.abs(audio_data))
        if max_val > 0:
            audio_data = audio_data / max_val * 0.8

        # Apply noise gate
        noise_floor = np.percentile(np.abs(audio_data), 10)
        gate_threshold = noise_floor * 3
        mask = np.abs(audio_data) > gate_threshold
        audio_data = audio_data * mask

        # Soft limiting
        if np.max(np.abs(audio_data)) > 1.0:
            audio_data = np.tanh(audio_data)

        return audio_data

    except Exception as e:
        logger.error(f"Error in audio preprocessing: {e}")
        return audio_data
```

#### Step 2: Buffer Management & Processing Trigger

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**In**: `process_audio_chunk()` (Line ~550)

```python
        # Add to buffer for processing (Line ~553)
        self.audio_buffer.append(audio_np)

        # Process accumulated audio (Line ~555)
        if len(self.audio_buffer) >= 3:  # Wait for 3 chunks = ~768ms
            # Combine last 5 chunks for context (Line ~556)
            combined_audio = np.concatenate(list(self.audio_buffer)[-5:])  # ~1.28s

            # Feed to AI models (Line ~559)
            await self.diarization.process_audio(combined_audio)
            await self.stt.process_audio(combined_audio)

            # Get NEW transcription only (Line ~562)
            new_transcription = self.stt.get_latest_transcription()
```

#### Step 3: Speaker Diarization

**File**: `AudioPipelineTreatment/diarization/speaker_diarization.py`
**Method**: `process_audio()` & `get_current_speaker()`

```python
class SpeakerDiarization:
    async def process_audio(self, audio_data):
        """Process audio for speaker identification"""
        # Voice Activity Detection (VAD)
        speech_segments = self.vad_model(audio_data)

        # Extract speaker embeddings
        if speech_segments:
            embeddings = self.extract_embeddings(audio_data, speech_segments)

            # Speaker clustering and identification
            speaker_id = self.cluster_speakers(embeddings)
            self.current_speaker = speaker_id

    def get_current_speaker(self):
        """Returns current speaker ID (0, 1, 2, etc.)"""
        return self.current_speaker
```

#### Step 4: Speech-to-Text Processing

**File**: `AudioPipelineTreatment/transcription/speech_to_text.py`
**Method**: `process_audio()` & `get_latest_transcription()`

```python
class SpeechToText:
    async def process_audio(self, audio_data):
        """Process audio with Whisper model"""
        # Whisper model inference
        result = self.whisper_model.transcribe(
            audio_data,
            language='en',
            task='transcribe',
            fp16=torch.cuda.is_available()
        )

        # Store new transcription
        self.latest_transcription = result['text'].strip()

    def get_latest_transcription(self):
        """Get ONLY the latest transcription (no accumulation)"""
        text = self.latest_transcription
        self.latest_transcription = ""  # Clear to prevent accumulation
        return text

    def reset(self):
        """Reset transcription state"""
        self.latest_transcription = ""
        self.accumulated_text = ""
```

#### Step 5: Voice Emotion Analysis

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**In**: `process_audio_chunk()` (Line ~575)

```python
            # Initialize speaker buffer if needed (Line ~575)
            if speaker not in self.speaker_audio_buffers:
                self.speaker_audio_buffers[speaker] = {
                    'buffer': [],
                    'last_analysis_time': 0
                }

            # Accumulate audio for voice analysis (Line ~582)
            self.speaker_audio_buffers[speaker]['buffer'].extend(combined_audio)

            # Keep last 2 seconds of audio (Line ~585)
            buffer_length = len(self.speaker_audio_buffers[speaker]['buffer'])
            if buffer_length > 2 * self.target_sr:
                keep_samples = int(0.5 * self.target_sr)
                self.speaker_audio_buffers[speaker]['buffer'] = \
                    self.speaker_audio_buffers[speaker]['buffer'][-keep_samples:]

            # Analyze voice if sufficient audio (Line ~592)
            current_time = time.time()
            if (len(analysis_buffer) >= self.target_sr and
                current_time - self.speaker_audio_buffers[speaker]['last_analysis_time'] > 1.5):

                voice_result = self.voice_analyzer.analyze(analysis_buffer)
                self.speaker_audio_buffers[speaker]['last_analysis_time'] = current_time
                self.last_voice_results[speaker] = voice_result
```

**File**: `AudioPipelineTreatment/sentiment/VoiceEmotionRecognizer.py`
**Method**: `analyze()`

```python
class VoiceEmotionRecognizer:
    def analyze(self, audio_data):
        """Analyze voice emotion from audio"""
        # Extract voice features
        pitch = self.extract_pitch(audio_data)
        energy = self.extract_energy(audio_data)
        speaking_rate = self.extract_speaking_rate(audio_data)

        # CNN-based emotion classification
        emotion_probabilities = self.emotion_model.predict(audio_data)
        emotion = self.get_dominant_emotion(emotion_probabilities)

        return {
            "voice": {
                "emotion": emotion,
                "score": emotion_probabilities.max(),
                "features": {
                    "pitch": float(pitch),
                    "energy": float(energy),
                    "speaking_rate": float(speaking_rate)
                }
            }
        }
```

#### Step 6: Text Sentiment Analysis

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**In**: `process_audio_chunk()` (Line ~607)

```python
            # Enhanced sentiment analysis (Line ~607)
            sentiment_result = self.sentiment_analyzer.analyze(text)
```

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Class**: `EnhancedSentimentAnalyzer` (Line ~169)

```python
class EnhancedSentimentAnalyzer:
    """Enhanced sentiment analyzer with manual overrides"""
    def __init__(self, base_analyzer):
        self.base_analyzer = base_analyzer

        # Manual overrides for misclassifications (Line ~173)
        self.negative_phrases = [
            'what the hell', 'totally unacceptable', 'worst', 'pathetic',
            'can\'t believe', 'fed up', 'sick of', 'this sucks'
        ]

    def analyze(self, text):
        """Enhanced sentiment with overrides"""
        # Get base sentiment from transformer model
        base_result = self.base_analyzer.analyze(text)

        text_lower = text.lower()

        # Override to NEGATIVE if negative phrases detected
        for phrase in self.negative_phrases:
            if phrase in text_lower:
                return {
                    'sentiment': 'NEGATIVE',
                    'confidence': 0.8,
                    'override_reason': f'Detected negative phrase: "{phrase}"'
                }

        # Check for profanity
        profanity_words = ['hell', 'damn', 'shit', 'fuck', 'crap']
        if any(word in text_lower for word in profanity_words):
            return {
                'sentiment': 'NEGATIVE',
                'confidence': 0.7,
                'override_reason': 'Profanity detected'
            }

        return base_result
```

#### Step 7: Comprehensive Emotion Analysis

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**In**: `process_audio_chunk()` (Line ~610)

```python
            # Enhanced emotion analysis (Line ~610)
            emotion_analysis = self.emotion_analyzer.analyze_emotion(
                text=text,
                text_sentiment=sentiment_result,
                voice_emotion=voice_result['voice']['emotion'],
                voice_features=voice_result['voice']['features'],
                voice_score=voice_result['voice']['score'],
                speaker_id=speaker
            )
```

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Class**: `EmotionAnalyzer` **Method**: `analyze_emotion()` (Line ~65)

```python
def analyze_emotion(self, text, text_sentiment, voice_emotion, voice_features, voice_score, speaker_id):
    """Comprehensive emotion analysis"""
    # Calculate voice intensity (Line ~68)
    voice_intensity = self._calculate_voice_intensity(voice_features)

    # Analyze text content (Line ~71)
    text_analysis = self._analyze_text_content(text)

    # Context analysis for escalation (Line ~74)
    context_score = self._analyze_context(speaker_id)

    # Initialize emotion analysis
    emotion_analysis = {
        'primary_emotion': 'NEUTRAL',
        'confidence': 0.0,
        'reasoning': [],
        'voice_intensity': voice_intensity,
        'text_indicators': text_analysis,
        'escalation_level': context_score
    }

    # Enhanced emotion detection logic (Line ~86)
    anger_score = self._calculate_anger_score(text_analysis, voice_intensity, text_sentiment, text)

    # Emotion classification (Line ~89)
    if anger_score >= 0.7:
        emotion_analysis['primary_emotion'] = 'ANGRY'
        emotion_analysis['confidence'] = min(anger_score, 0.95)
    elif anger_score >= 0.5:
        emotion_analysis['primary_emotion'] = 'FRUSTRATED'
        emotion_analysis['confidence'] = anger_score
    elif text_sentiment.get('sentiment') == "NEGATIVE" and voice_intensity > 0.4:
        emotion_analysis['primary_emotion'] = 'IRRITATED'
        emotion_analysis['confidence'] = 0.6
    elif text_sentiment.get('sentiment') == "POSITIVE":
        emotion_analysis['primary_emotion'] = 'SATISFIED'
        emotion_analysis['confidence'] = 0.6

    return emotion_analysis
```

#### Step 8: Result Compilation & Transmission

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**In**: `process_audio_chunk()` (Line ~620)

```python
            # Create result for frontend (Line ~620)
            timestamp = datetime.now().strftime("%H:%M:%S")

            result = {
                "timestamp": timestamp,
                "speaker": speaker_id,  # "SPEAKER_00", "SPEAKER_01"
                "text": text,
                "sentiment": {
                    "text": sentiment_result.get("sentiment", "NEUTRAL"),
                    "voice": emotion_analysis['primary_emotion'],
                    "score": emotion_analysis['confidence']
                },
                "voiceFeatures": voice_result['voice']['features'],
                "sessionId": self.session_id,
                "analysisType": "real-time",
                "emotionAnalysis": emotion_analysis,
                "audioStats": self.audio_stats
            }

            # Reset STT to prevent accumulation (Line ~640)
            self.stt.reset()

            # Clear audio buffer to prevent overlap (Line ~643)
            self.audio_buffer.clear()

            return result
```

**Method**: `send_result()` (Line ~695)

```python
async def send_result(self, result: Dict):
    """Send analysis result to frontend via WebSocket"""
    if self.websocket and result:
        try:
            # Custom JSON serialization for numpy/torch types
            await self.websocket.send_text(safe_json_dumps(result))
        except Exception as e:
            logger.error(f"Error sending result to WebSocket: {e}")
```

### Frontend Result Processing - **EXACT CODE IMPLEMENTATION**

#### Step 1: WebSocket Message Reception

**File**: `FrontEnd/src/app/services/audio-stream.service.ts`
**Method**: `handleMessage()`

```typescript
private handleMessage(event: MessageEvent): void {
  try {
    const data = JSON.parse(event.data);

    // Emit to component for processing
    this.onAnalysisResult.emit(data);

  } catch (error) {
    console.error('Error parsing WebSocket message:', error);
  }
}
```

#### Step 2: Component Result Processing

**File**: `FrontEnd/src/app/Agent/call-managment/call-managment.component.ts`
**Method**: `setupAudioStreamSubscriptions()` (Line ~150)

```typescript
private setupAudioStreamSubscriptions() {
  // Subscribe to analysis results
  this.audioStreamService.onAnalysisResult.subscribe((result) => {
    this.processAnalysisResult(result);
  });
}

private processAnalysisResult(result: any) {
  // Deduplication check to prevent overlapping text
  const isDuplicate = this.analysisResults.some(existing =>
    existing.text === result.text &&
    existing.speaker === result.speaker &&
    Math.abs(new Date(existing.timestamp).getTime() - new Date(result.timestamp).getTime()) < 5000
  );

  if (!isDuplicate) {
    // Add to results array
    this.analysisResults.push(result);

    // Trigger Angular change detection for UI update
    this.cdr.detectChanges();

    // Scroll to newest result
    setTimeout(() => this.scrollToBottom(), 100);
  }
}
```

#### Step 3: UI Rendering with Sneat Badges

**File**: `FrontEnd/src/app/Agent/call-managment/call-managment.component.html`
**Template Section** (Line ~272):

```html
<!-- Sentiment Display with Colorful Badges -->
<div class="d-flex align-items-center gap-2">
  <span class="fs-5">{{ getSentimentIcon(result.sentiment.text) }}</span>
  <span
    class="badge rounded-pill"
    [class]="getSentimentBadgeClass(result.sentiment.text)"
  >
    {{ result.sentiment.text | titlecase }}
  </span>
  <small class="text-muted"
    >({{ result.sentiment.score * 100 | number : "1.0-0" }}%)</small
  >
</div>

<!-- Voice Features with Colorful Badges -->
<div class="d-flex gap-2 flex-wrap">
  <span class="badge bg-label-info">
    <i class="bx bx-microphone me-1"></i>
    <strong>Voice:</strong>
    {{ result.sentiment.voice || "NEUTRAL" }}
  </span>
  <span class="badge bg-label-warning">
    <i class="bx bx-trending-up me-1"></i>
    <strong>Pitch:</strong>
    {{ (result.voiceFeatures && result.voiceFeatures.pitch) || 0.0 | number :
    "1.2-2" }}
  </span>
  <span class="badge bg-label-success">
    <i class="bx bx-pulse me-1"></i>
    <strong>Energy:</strong>
    {{ (result.voiceFeatures && result.voiceFeatures.energy) || 0.0 | number :
    "1.2-2" }}
  </span>
</div>
```

**Badge Color Logic** (Line ~365):

```typescript
getSentimentBadgeClass(sentiment: string): string {
  switch (sentiment.toLowerCase()) {
    case 'positive':
    case 'satisfied':
    case 'enthusiastic':
      return 'bg-success';  // Green
    case 'negative':
    case 'disappointed':
    case 'angry':
    case 'frustrated':
    case 'irritated':
      return 'bg-danger';   // Red
    case 'neutral':
      return 'bg-secondary'; // Gray
    default:
      return 'bg-secondary';
  }
}
```

---

## üìä **PHASE 3: CONTINUOUS MONITORING (Throughout Session)**

### Performance Metrics Tracking - **EXACT CODE LOCATION**

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Property**: `audio_stats` (Updated in `process_audio_chunk()`)

```python
# Performance tracking (Line ~645 in process_audio_chunk)
self.audio_stats['chunks_processed'] += 1
self.audio_stats['total_audio_duration'] += len(audio_np) / self.target_sr

# Track speech activity
if len(text) >= 3:
    self.audio_stats['chunks_with_speech'] += 1
    self.audio_stats['speech_duration'] += len(combined_audio) / self.target_sr

# Volume statistics
current_volume = float(np.mean(np.abs(audio_np)))
self.audio_stats['avg_volume'] = (
    self.audio_stats['avg_volume'] * 0.9 + current_volume * 0.1
)
if current_volume > self.audio_stats['peak_volume']:
    self.audio_stats['peak_volume'] = current_volume
```

### Memory Management - **EXACT CODE IMPLEMENTATIONS**

#### Audio Buffer Management

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Buffer**: `audio_buffer` (Line ~42)

```python
# Circular buffer initialization (Line ~42)
self.audio_buffer = deque(maxlen=1000)  # Prevents infinite growth

# Buffer clearing after processing (Line ~643)
self.audio_buffer.clear()  # Prevent audio overlap
```

#### Speaker Buffer Cleanup

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**In**: `process_audio_chunk()` (Line ~585)

```python
# Keep only last 2 seconds per speaker (Line ~585)
buffer_length = len(self.speaker_audio_buffers[speaker]['buffer'])
if buffer_length > 2 * self.target_sr:  # 2 seconds at target sample rate
    keep_samples = int(0.5 * self.target_sr)  # Keep last 0.5 seconds
    self.speaker_audio_buffers[speaker]['buffer'] = \
        self.speaker_audio_buffers[speaker]['buffer'][-keep_samples:]
```

#### GPU Memory Management

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Method**: `cleanup_gpu_memory()` (Line ~748)

```python
def cleanup_gpu_memory(self):
    """Clean up GPU memory"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
```

### Error Handling - **EXACT CODE IMPLEMENTATIONS**

#### WebSocket Connection Monitoring

**File**: `FrontEnd/src/app/services/audio-stream.service.ts`
**Error Handlers**:

```typescript
// Connection error handling
this.socket.onerror = (error) => {
  console.error("‚ùå WebSocket error:", error);
  this.connectionStatus = "Error";
  this.isConnected = false;
};

// Connection close handling
this.socket.onclose = (event) => {
  console.log("üîå WebSocket closed:", event.code, event.reason);
  this.isConnected = false;

  // Attempt reconnection if unexpected
  if (!event.wasClean) {
    this.attemptReconnection();
  }
};
```

#### Backend Error Recovery

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**In**: `process_audio_chunk()` (Line ~670)

```python
    except Exception as e:
        logger.error(f"Error in audio processing pipeline: {str(e)}")
        logger.error(f"Error type: {type(e).__name__}")
        logger.error(f"Traceback: {traceback.format_exc()}")

        # Graceful degradation - continue session but log error
        self.audio_stats['errors'] = self.audio_stats.get('errors', 0) + 1

        # Send error notification to frontend
        if self.websocket:
            await self.websocket.send_text(safe_json_dumps({
                "type": "processing_error",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }))

        return None
```

---

## üõë **PHASE 4: TERMINATION (When you click "Stop Analysis")**

### Frontend Shutdown - **EXACT CODE FLOW**

#### Step 1: Stop Button Click

**File**: `FrontEnd/src/app/Agent/call-managment/call-managment.component.html`
**Button** (Line ~127):

```html
<button
  *ngIf="isAnalyzing"
  (click)="stopAnalysis()"
  class="btn btn-outline-secondary"
>
  <i class="bx bx-stop-circle me-1"></i>
  Stop Analysis
</button>
```

#### Step 2: Component Stop Method

**File**: `FrontEnd/src/app/Agent/call-managment/call-managment.component.ts`
**Method**: `stopAnalysis()` (Line ~325)

```typescript
async stopAnalysis() {
  try {
    this.connectionStatus = 'Stopping...';

    // Stop audio recording but keep WebSocket open
    this.audioStreamService.stopRecording();

    this.isAnalyzing = false;
    this.connectionStatus = 'Stopped';

    console.log('‚úÖ Enhanced analysis stopped');
  } catch (error) {
    console.error('‚ùå Error stopping analysis:', error);
    this.connectionStatus = 'Error stopping: ' + error;
  }
}
```

#### Step 3: Audio Stream Service Shutdown

**File**: `FrontEnd/src/app/services/audio-stream.service.ts`
**Method**: `stopRecording()`

```typescript
stopRecording(): void {
  try {
    // Disconnect audio processing chain
    if (this.scriptProcessor) {
      this.scriptProcessor.disconnect();
      this.scriptProcessor = null;
    }

    if (this.mediaStreamSource) {
      this.mediaStreamSource.disconnect();
      this.mediaStreamSource = null;
    }

    // Stop microphone stream
    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => {
        track.stop();
        console.log('üé§ Audio track stopped');
      });
      this.mediaStream = null;
    }

    // Close audio context
    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }

    // Note: WebSocket connection kept open for cleanup messaging
    console.log('üîá Recording stopped, keeping WebSocket open');

  } catch (error) {
    console.error('‚ùå Error stopping recording:', error);
  }
}
```

### Backend Cleanup - **EXACT CODE FLOW**

#### Session Stop Handling

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Method**: `stop_session()` (Line ~713)

```python
async def stop_session(self):
    """Stop the current audio processing session"""
    if not self.is_running:
        logger.warning("No active session to stop")
        return False

    logger.info(f"Stopping session: {self.session_id}")

    # Mark session as stopped (Line ~720)
    self.is_running = False

    # Clear all audio buffers (Line ~723)
    self.audio_buffer.clear()
    if hasattr(self, 'speaker_audio_buffers'):
        for speaker_id in self.speaker_audio_buffers:
            self.speaker_audio_buffers[speaker_id]['buffer'].clear()
        self.speaker_audio_buffers.clear()

    # Clear voice analysis cache (Line ~730)
    if hasattr(self, 'last_voice_results'):
        self.last_voice_results.clear()

    # Reset STT component (Line ~734)
    if self.stt:
        self.stt.reset()

    # Clean up GPU memory (Line ~737)
    self.cleanup_gpu_memory()

    # Calculate final statistics (Line ~740)
    final_stats = self.audio_stats.copy()
    final_stats['session_duration'] = time.time() - self.session_start_time

    # Send final confirmation to frontend (Line ~744)
    if self.websocket:
        await self.websocket.send_text(safe_json_dumps({
            "type": "session_stopped",
            "sessionId": self.session_id,
            "finalStats": final_stats,
            "timestamp": datetime.now().isoformat()
        }))

    logger.info("Session stopped successfully")
    return True
```

---

## üîÑ **KEY CODE OPTIMIZATIONS & FEATURES**

### Performance Optimizations - **CODE LOCATIONS**

#### 1. No WAV File Generation

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Line**: ~40

```python
self.recording_enabled = False  # DISABLED - no disk I/O
```

#### 2. Efficient Buffering System

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Buffer Initialization** (Line ~42):

```python
self.audio_buffer = deque(maxlen=1000)  # Circular buffer
```

**Optimal Chunk Processing** (Line ~555):

```python
if len(self.audio_buffer) >= 3:  # Wait for 3 chunks = ~768ms
    combined_audio = np.concatenate(list(self.audio_buffer)[-5:])  # Use last 5 for context
```

#### 3. Smart Voice Analysis Caching

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Caching Logic** (Line ~592):

```python
# Analyze voice only every 1.5 seconds per speaker
if (len(analysis_buffer) >= self.target_sr and
    current_time - self.speaker_audio_buffers[speaker]['last_analysis_time'] > 1.5):

    voice_result = self.voice_analyzer.analyze(analysis_buffer)
    self.last_voice_results[speaker] = voice_result  # Cache result
```

#### 4. GPU Memory Optimization

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**GPU Cleanup** (Line ~748):

```python
def cleanup_gpu_memory(self):
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
```

### Anti-Repetition Mechanisms - **CODE IMPLEMENTATIONS**

#### 1. STT Reset After Each Processing

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Line**: ~640

```python
# Reset STT to prevent text accumulation
self.stt.reset()
```

**File**: `AudioPipelineTreatment/transcription/speech_to_text.py`
**Method**: `reset()`

```python
def reset(self):
    """Reset transcription state to prevent accumulation"""
    self.latest_transcription = ""
    self.accumulated_text = ""
    # Clear any internal Whisper state
```

#### 2. Latest Text Only Extraction

**File**: `AudioPipelineTreatment/transcription/speech_to_text.py`
**Method**: `get_latest_transcription()`

```python
def get_latest_transcription(self):
    """Get ONLY the latest transcription (no accumulation)"""
    text = self.latest_transcription
    self.latest_transcription = ""  # Clear immediately
    return text
```

#### 3. Buffer Clearing After Processing

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`
**Line**: ~643

```python
# Clear audio buffer to prevent overlap
self.audio_buffer.clear()
```

#### 4. Frontend Deduplication

**File**: `FrontEnd/src/app/Agent/call-managment/call-managment.component.ts`
**Method**: `processAnalysisResult()`

```typescript
private processAnalysisResult(result: any) {
  // Prevent duplicate results
  const isDuplicate = this.analysisResults.some(existing =>
    existing.text === result.text &&
    existing.speaker === result.speaker &&
    Math.abs(new Date(existing.timestamp).getTime() - new Date(result.timestamp).getTime()) < 5000
  );

  if (!isDuplicate) {
    this.analysisResults.push(result);
    this.cdr.detectChanges();
  }
}
```

### Real-Time UI Features - **CODE IMPLEMENTATIONS**

#### 1. Live Sentiment Badge Colors

**File**: `FrontEnd/src/app/Agent/call-managment/call-managment.component.ts`
**Method**: `getSentimentBadgeClass()` (Line ~365)

```typescript
getSentimentBadgeClass(sentiment: string): string {
  switch (sentiment.toLowerCase()) {
    case 'positive':
    case 'satisfied':
    case 'enthusiastic':
      return 'bg-success';    // Bootstrap green
    case 'negative':
    case 'angry':
    case 'frustrated':
    case 'irritated':
      return 'bg-danger';     // Bootstrap red
    case 'neutral':
      return 'bg-secondary';  // Bootstrap gray
    default:
      return 'bg-secondary';
  }
}
```

#### 2. Smooth Auto-Scroll

**File**: `FrontEnd/src/app/Agent/call-managment/call-managment.component.ts`
**Method**: `scrollToBottom()` (Line ~420)

```typescript
scrollToBottom(): void {
  try {
    const container = document.querySelector('.results-container');
    if (container) {
      container.scrollTop = container.scrollHeight;
    }
  } catch (err) {
    console.error('Scroll error:', err);
  }
}
```

#### 3. Voice Analytics Badges

**File**: `FrontEnd/src/app/Agent/call-managment/call-managment.component.html`
**Template** (Line ~295):

```html
<!-- Voice Features Display -->
<div class="d-flex gap-2 flex-wrap">
  <span class="badge bg-label-info">
    <i class="bx bx-microphone me-1"></i>
    <strong>Voice:</strong> {{ result.sentiment.voice || "NEUTRAL" }}
  </span>
  <span class="badge bg-label-warning">
    <i class="bx bx-trending-up me-1"></i>
    <strong>Pitch:</strong> {{ result.voiceFeatures?.pitch | number:'1.2-2' }}
  </span>
  <span class="badge bg-label-success">
    <i class="bx bx-pulse me-1"></i>
    <strong>Energy:</strong> {{ result.voiceFeatures?.energy | number:'1.2-2' }}
  </span>
</div>
```

---

## üìà **COMPLETE DATA FLOW WITH FILE REFERENCES**

### Audio Data Journey

```
üé§ Microphone Input
    ‚Üì (4096 samples, 16kHz, Float32Array)
üì± FrontEnd/src/app/services/audio-stream.service.ts - processAudioData()
    ‚Üì (Convert to Int16 PCM, WebSocket binary send)
üåê WebSocket Protocol (ws://localhost:8000/ws)
    ‚Üì (Raw PCM bytes)
üêç AudioPipelineTreatment/enhanced_web_realtime_pipeline.py - process_audio_chunk()
    ‚Üì (Convert to numpy, preprocess, buffer)
üß† AI Models Processing:
    ‚Ä¢ AudioPipelineTreatment/transcription/speech_to_text.py (Whisper STT)
    ‚Ä¢ AudioPipelineTreatment/diarization/speaker_diarization.py (PyAnnote)
    ‚Ä¢ AudioPipelineTreatment/sentiment/ (Sentiment + Voice Emotion)
    ‚Üì (Combined AI results)
üìä Result Compilation (enhanced_web_realtime_pipeline.py)
    ‚Üì (JSON with safe serialization)
üåê WebSocket JSON Response
    ‚Üì (Parsed result object)
üéØ FrontEnd/src/app/Agent/call-managment/call-managment.component.ts - processAnalysisResult()
    ‚Üì (Add to results array, trigger change detection)
üñºÔ∏è FrontEnd/src/app/Agent/call-managment/call-managment.component.html - UI Update
    ‚Üì (Colorful badges, voice analytics, smooth scroll)
üëÄ User Sees Real-Time Analysis Results
```

### Timeline Performance

```
Audio Chunk Duration: ~256ms (4096 samples @ 16kHz)
Processing Latency: ~200-500ms (AI model inference)
Total User-Perceived Delay: ~500-800ms
Update Frequency: 2-3 results/second during active speech
Buffer Context: ~1.28s (5 chunks combined for AI processing)
Memory Usage: Circular buffers prevent growth
```

---

## üõ†Ô∏è **TECHNICAL SPECIFICATIONS WITH CODE REFERENCES**

### Audio Processing Configuration

**File**: `AudioPipelineTreatment/enhanced_web_realtime_pipeline.py`

```python
# Audio settings (Line ~35)
self.target_sr = 16000  # 16kHz sample rate
self.chunk_size = 4096  # ~256ms chunks at 16kHz
self.recording_enabled = False  # No WAV file generation
```

**File**: `FrontEnd/src/app/services/audio-stream.service.ts`

```typescript
// Frontend audio settings
audio: {
  sampleRate: 16000,     // Match backend
  channelCount: 1,       // Mono
  echoCancellation: true,
  noiseSuppression: true,
  autoGainControl: true
}
```

### AI Model Specifications

```
1. Whisper STT: openai/whisper-base.en
   Location: AudioPipelineTreatment/transcription/speech_to_text.py

2. PyAnnote Diarization: pyannote/speaker-diarization
   Location: AudioPipelineTreatment/diarization/speaker_diarization.py

3. Sentiment Analysis: transformers pipeline
   Location: AudioPipelineTreatment/sentiment/

4. Voice Emotion: Custom CNN model
   Location: AudioPipelineTreatment/sentiment/VoiceEmotionRecognizer.py

5. Enhanced Emotion: Rule-based + ML fusion
   Location: AudioPipelineTreatment/enhanced_web_realtime_pipeline.py (Line ~65)
```

### WebSocket Protocol Details

```
Connection: ws://localhost:8000/ws
Binary Messages: Raw PCM audio (Int16, 16kHz, Mono)
Text Messages: JSON results with custom numpy/torch serialization
Heartbeat: Automatic reconnection on connection loss
Error Handling: Graceful degradation with error notifications
```

This comprehensive documentation shows exactly what happens at every step, with precise file locations and line numbers so you can trace the complete flow through your codebase.
